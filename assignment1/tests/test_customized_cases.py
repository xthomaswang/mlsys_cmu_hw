"""We encourage you to create your own test cases, which helps
you confirm the correctness of your implementation.

If you are interested, you can write your own tests in this file
and share them with us by including this file in your submission.
Please make the tests "pytest compatible" by starting each test
function name with prefix "test_".

We appreciate it if you can share your tests, which can help
improve this course and the assignment. However, please note that
this part is voluntary -- you will not get more scores by sharing
test cases, and conversely, will not get fewer scores if you do
not share.
"""
#generated by AI-tool
from typing import Dict, List

import numpy as np
import auto_diff as ad


def check_evaluator_output(
    evaluator: ad.Evaluator,
    input_values: Dict[ad.Node, np.ndarray],
    expected_outputs: List[np.ndarray],
) -> None:
    """
    与原有测试代码相同，用于检验前向和后向传播结果。
    """
    output_values = evaluator.run(input_values)
    assert len(output_values) == len(expected_outputs)
    for output_val, expected_val in zip(output_values, expected_outputs):
        np.testing.assert_allclose(
            actual=output_val,
            desired=expected_val,
            rtol=1e-4,
            atol=1e-6,
            err_msg=f"Output mismatch.\nGot:\n{output_val}\nExpected:\n{expected_val}"
        )

def test_graph():
    # 定义计算图
    x1 = ad.Variable("x1")  # shape: (1,1)
    x2 = ad.Variable("x2")  # shape: (1,1)
    x3 = ad.Variable("x3")  # shape: (1,1)

    # y = (x1 * x2) / 10 * x3
    # 注意：在你的框架里是 matmul(x1, x2, trans_B=True)，
    # 但当 x1, x2 形状均为 (1,1) 时，这等价于标量乘法。
    y = ad.matmul(x1, x2, trans_B=True) / 10 * x3

    # 对 x1, x2, x3 分别求梯度
    x1_grad, x2_grad, x3_grad = ad.gradients(y, nodes=[x1, x2, x3])

    # 构造评估器
    evaluator = ad.Evaluator(eval_nodes=[x1_grad, x2_grad, x3_grad])

    # 准备简单的输入数据
    # 让 x1=2, x2=3, x3=4 (都用形状(1,1))
    # 那么正向:
    #   y = (2 * 3)/10 * 4 = 6/10 * 4 = 2.4
    #
    # 计算各自梯度:
    #   dy/dx1 = x3 * (x2/10) = 4 * (3/10) = 1.2
    #   dy/dx2 = x3 * (x1/10) = 4 * (2/10) = 0.8
    #   dy/dx3 = (x1 * x2)/10 = 6/10 = 0.6
    #
    check_evaluator_output(
        evaluator,
        input_values={
            x1: np.array([[2.]], dtype=np.float32),
            x2: np.array([[3.]], dtype=np.float32),
            x3: np.array([[4.]], dtype=np.float32),
        },
        expected_outputs=[
            np.array([[1.2]], dtype=np.float32),  # x1_grad
            np.array([[0.8]], dtype=np.float32),  # x2_grad
            np.array([[0.6]], dtype=np.float32),  # x3_grad
        ],
    )


def test_gradient_of_gradient():
    x1 = ad.Variable(name="x1")  # shape: (2,2)
    x2 = ad.Variable(name="x2")  # shape: (2,2)
    # y = x1^2 + x1 * x2  (逐元素)
    y = x1 * x1 + x1 * x2

    # 一阶梯度
    grad_x1, grad_x2 = ad.gradients(y, [x1, x2])
    # 对 grad_x1 再分别对 x1, x2 求梯度
    grad_x1_x1, grad_x1_x2 = ad.gradients(grad_x1, [x1, x2])
    # 对 grad_x2 再分别对 x1, x2 求梯度
    grad_x2_x1, grad_x2_x2 = ad.gradients(grad_x2, [x1, x2])

    evaluator = ad.Evaluator([
        y,             # 0
        grad_x1,       # 1
        grad_x2,       # 2
        grad_x1_x1,    # 3
        grad_x1_x2,    # 4
        grad_x2_x1,    # 5
        grad_x2_x2,    # 6
    ])

    # 准备更简单的输入:
    x1_val = np.array([
        [1., 2.],
        [3., 4.]
    ], dtype=np.float32)
    x2_val = np.array([
        [2., 1.],
        [0., 3.]
    ], dtype=np.float32)

    check_evaluator_output(
        evaluator,
        input_values={
            x1: x1_val,
            x2: x2_val,
        },
        expected_outputs=[
            # y = [[3, 6], [9, 28]]
            np.array([[3.,  6.],
                      [9., 28.]], dtype=np.float32),
            # grad_x1 = 2*x1 + x2 = [[4,5],[6,11]]
            np.array([[4.,  5.],
                      [6., 11.]], dtype=np.float32),
            # grad_x2 = x1 = [[1,2],[3,4]]
            np.array([[1., 2.],
                      [3., 4.]], dtype=np.float32),
            # grad_x1_x1 = 2 everywhere => [[2,2],[2,2]]
            2. * np.ones((2,2), dtype=np.float32),
            # grad_x1_x2 = 1 everywhere => [[1,1],[1,1]]
            1. * np.ones((2,2), dtype=np.float32),
            # grad_x2_x1 = 1 everywhere => [[1,1],[1,1]]
            1. * np.ones((2,2), dtype=np.float32),
            # grad_x2_x2 = 0 => [[0,0],[0,0]]
            np.zeros((2,2), dtype=np.float32),
        ],
    )


if __name__ == "__main__":
    test_graph()
    test_gradient_of_gradient()