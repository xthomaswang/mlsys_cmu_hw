{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPrCLeqDcia_"
   },
   "source": [
    "# Machine Learning Systems Assignment 1\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlsyscourse/assignment1/blob/main/mlsys_hw1.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "**Assignment due: Feb 10, 2025, 11:59 pm, Eastern Time**.\n",
    "\n",
    "Automatic differentiation is the foundation technique of training a machine learning model.\n",
    "In this assignment, you will implement a simple prototype automatic differentiation system (learned in lecture 4), build up your own logistics regression model, and train the model on a handwritten digit dataset.\n",
    "\n",
    "* You should work on this assignment **individually** -- it is not a team assignment.\n",
    "* This assignment does not require GPU. You can do the assignment on either Google Colab (by clicking the badge above), your laptop/desktop, or any server that you have access to.\n",
    "* This assignment is pure Python. No C++ is needed in this assignment.\n",
    "* Please check out the end of this notebook for the assignment submission requirement.\n",
    "* Please do not share your solution on publicly available websites (e.g., GitHub).\n",
    "* **About testing and grading.** The assignment will be automatically graded. The test cases include both public tests that we provide under `tests/`, as well as some private tests (which will not be disclosed). You can submit multiple times, and the time stamp of that submission will be used in determining any late penalties. The scores you get in each task is proportional to the number of total test cases you pass. We also encourage you to create your own test cases, which helps you confirm the correctness of your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDVuWaftK_h3"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "* If you are using Google Colab environment, please make a copy of this notebook file by selecting \"Save a copy in Drive\" from the \"File\" menu, and then run the code block below to set up workspace. After cloning, you will see the cloned repository in the \"Files\" bar on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2864,
     "status": "ok",
     "timestamp": 1706413992841,
     "user": {
      "displayName": "Ruihang Lai",
      "userId": "03605576729730874720"
     },
     "user_tz": 300
    },
    "id": "hx2QcOuFR6Nu",
    "outputId": "dd2dccc8-63fe-424b-c2cd-5077321470a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive\n",
      "/content/drive/MyDrive/15442\n",
      "Cloning into 'assignment1'...\n",
      "remote: Enumerating objects: 105, done.\u001b[K\n",
      "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
      "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
      "remote: Total 105 (delta 52), reused 105 (delta 52), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (105/105), 274.29 KiB | 3.97 MiB/s, done.\n",
      "Resolving deltas: 100% (52/52), done.\n",
      "/content/drive/MyDrive/15442/assignment1\n"
     ]
    }
   ],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 15442\n",
    "%cd /content/drive/MyDrive/15442\n",
    "!git clone https://github.com/mlsyscourse/assignment1.git\n",
    "%cd /content/drive/MyDrive/15442/assignment1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITXgDFZSK_h5"
   },
   "source": [
    "* If you are using local/server environment, please clone this repository.\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/mlsyscourse/assignment1.git\n",
    "cd assignment1\n",
    "export PYTHONPATH=.:$PYTHONPATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eWqMr7QK_h5"
   },
   "source": [
    "## Part 1: Automatic Differentiation Framework (60 pt)\n",
    "\n",
    "In part 1, you will implement the reverse mode automatic differentiation algorithm.\n",
    "\n",
    "The auto diff algorithm in this assignment works on a **computational graph**.\n",
    "A computational graph describes the process of computation of an expression.\n",
    "For example, given $x_1$, $x_2$, the expression $y = x_1 \\times x_2 + x_1$ has the following computational graph:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mlsyscourse/assignment1/main/figure/computational_graph.jpg\" alt=\"figure/computational_graph.jpg\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOLthvm3K_h6"
   },
   "source": [
    "Let's first walk you through the basic concepts and data structures in the framework.\n",
    "A computational graph consists of **nodes**, where each node denotes an intermediate step of computation during computing the entire expression.\n",
    "Every node is composed of the three parts (`auto_diff.py` line 6):\n",
    "\n",
    "- an **operation** (field `op`), which defines the operation that the node computes.\n",
    "- a list of **input nodes** (field `inputs`), which indicates the input source of the computation.\n",
    "- optionally, additional \"**attributes**\" (field `attrs`). The attributes that a node has depends on the op of the node. We will explain the attributes later in this part.\n",
    "\n",
    "We can define an input node of a computational graph with `ad.Variable`. For example, the input variable nodes $x_1$ and $x_2$ can be defined as\n",
    "\n",
    "```python\n",
    "import auto_diff as ad\n",
    "\n",
    "x1 = ad.Variable(name=\"x1\")\n",
    "x2 = ad.Variable(name=\"x2\")\n",
    "```\n",
    "\n",
    "In `auto_diff.py` (line 81), you can see that the essence of `ad.Variable` is to construct a node\n",
    "with op `placeholder` and the given name. The input nodes have empty `inputs` and `attrs`:\n",
    "```python\n",
    "class Variable(Node):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(inputs=[], op=placeholder, name=name)\n",
    "```\n",
    "\n",
    "Here, the `placeholder` defines the computation of a input variable node, which is \"doing nothing.\"\n",
    "Besides `placeholder`, we have other ops defined in `auto_diff.py`. For example,\n",
    "\n",
    "- op `add` defines the addition of two nodes,\n",
    "- op `matmul` defines the matrix multiplication of two nodes.\n",
    "\n",
    "Notably, these ops are globally defined for only once, and the `op` field of every node is such\n",
    "a globally defined op."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAcv8S6NK_h6"
   },
   "source": [
    "Now, back to our example of $y = x_1 \\times x_2 + x_1$.\n",
    "Now that we have `x1` and `x2` as two input variable nodes, we can define the rest of the computational graph\n",
    "with a one-line Python code:\n",
    "```python\n",
    "y = x1 * x2 + x1\n",
    "```\n",
    "\n",
    "This line first constructs a node with op `mul` (multiplication) and `x1`, `x2` as `inputs`,\n",
    "and then constructs a node with op `add` which takes the previous multiplication node and `x1` as `inputs`.\n",
    "As a result, our computational graph contains four nodes in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYS8MfifK_h6"
   },
   "source": [
    "It worths noting that a computational graph (e.g., the four nodes we defined) **does not** carry concrete values of nodes.\n",
    "The style of this assignment is consistent with the TensorFlow v1 style, introduced in the lecture.\n",
    "This is different from frameworks like PyTorch, where the values of input tensors will be given in the beginning,\n",
    "and the values of intermediate tensors are eagerly computed along the way when those tensors are defined.\n",
    "In our computational graph, to compute the value of output `y` given values of input `x1`, `x2`,\n",
    "we provide the `Evaluator` class (`auto_diff.py` line 373)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCip5YB7K_h6"
   },
   "source": [
    "Here is an example of how `Evaluator` works. The constructor of `Evaluator` takes a list of nodes to evaluate.\n",
    "By writing\n",
    "```python\n",
    "evaluator = ad.Evaluator(eval_nodes=[y])\n",
    "```\n",
    "it means that we construct an `Evaluator` instance which aims to compute the value of `y`.\n",
    "Then we provide the values (assuming all `numpy.ndarray` in this assignment) of the input tensors through the main interface `Evaluator.run` (which you need to implement):\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "x1_value = np.array(2)\n",
    "x2_value = np.array(3)\n",
    "y_value = evaluator.run(input_dict={x1: x1_value, x2: x2_value})\n",
    "```\n",
    "\n",
    "At a high level, here the `run` method consumes the input values via a dictionary `Dict[Node, numpy.ndarray]`,\n",
    "computes the value of node `y` internally, and returns the result.\n",
    "Given `2 * 3 + 2 = 8`, the returned `y_value` should be `np.ndarray(8)` eventually (of course, it will not return the correct value before your finish implementing):\n",
    "```python\n",
    "np.testing.assert_allclose(y_value, np.array(8))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LMnlNGTK_h7"
   },
   "source": [
    "`Evaluator.run` method effectively computes the forward computation of nodes, and we can go ahead to talk about the backward.\n",
    "As you learned in the lecture, in order to compute the output gradient with regard to each input node in a computational graph,\n",
    "we can extend the forward graph with the additional backward part.\n",
    "Once we have the forward and backward graph together, by given input node values,\n",
    "we can use `Evaluator` to compute the output value, loss value, and the gradient values of each input nodes altogether with a single-time `Evaluator.run`.\n",
    "\n",
    "The function `gradients(output_node: Node, nodes: List[Node]) -> List[Node]` in `auto_diff.py` is\n",
    "the function you need to implement to construct the backward graph.\n",
    "This function takes an output node (usually the node of the loss function in machine learning), whose gradient is treated as 1,\n",
    "takes the list of nodes to compute gradients for,\n",
    "and returns the gradient nodes with regard to each node in the input list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjz5vvUcK_h7"
   },
   "source": [
    "Back to our example, after implementing `gradients`, you can run\n",
    "```python\n",
    "x1_grad, x2_grad = ad.gradients(output_node=y, node=[x1, x2])\n",
    "```\n",
    "to get the gradients of $y$ regarding $x_1$ and $x_2$ respectively.\n",
    "And you can construct `Evaluator` on nodes `y`, `x1_grad` and `x2_grad`, and use `Evaluator.run`\n",
    "to compute the output value and input gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9iDUNiYK_h7"
   },
   "source": [
    "Finally, before leaving the assignment to you, we introduce how op works.\n",
    "As you can find in `auto_diff.py`, each op defines three methods:\n",
    "\n",
    "- `__call__(self, **kwargs) -> Node`, which takes in the input nodes (and attributes), constructs a new node with this op, and returns the constructed node.\n",
    "- `compute(self, node: Node, input_values: List[np.ndarray]) -> np.ndarray`, which takes the node to compute with its input values, and returns the computed value of the given node.\n",
    "- `gradient(self, node: Node, output_grad: Node) -> List[Node]`, which takes a node and the gradient node of this node, and returns the nodes of the input partial adjoints (one for each input node).\n",
    "\n",
    "In general, the `Op.compute` method computes the value of a single node with given node inputs, and `Evaluator.run` function computes the value of a graph output with given graph inputs.\n",
    "`Op.gradient` method constructs the backward computational graph for a single node, and `gradients` function constructs the backward graph for a graph.\n",
    "That being said, your implementation of `Evaluator.run` should effectively make use of the `compute` method of op,\n",
    "and likewise, the `gradients` function implementation should leverage the `gradient` method defined in op."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moOEW6aNK_h7"
   },
   "source": [
    "### Your tasks\n",
    "\n",
    "**Task 1 (10 pt).** Implement the `compute` method for all ops in `auto_diff.py`. We provide the examples of `AddOp` and `AddByConstOp`, and you need to implement the rest.\n",
    "For this assignment, you can assume that the inputs of addition/multiplication/division have the same shape.\n",
    "\n",
    "We provide sample tests in `tests/test_auto_diff_node_forward.py`.\n",
    "You can test your task 1 implementation by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1146,
     "status": "ok",
     "timestamp": 1706414004471,
     "user": {
      "displayName": "Ruihang Lai",
      "userId": "03605576729730874720"
     },
     "user_tz": 300
    },
    "id": "13URNWeEK_h8",
    "outputId": "a75d39e6-e114-455a-cbb8-44c1fc793ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.3.4, pluggy-1.5.0 -- /Users/tuomasier/miniconda3/envs/mlsys/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/tuomasier/Desktop/mlsys/assignment1\n",
      "plugins: anyio-4.6.2\n",
      "collected 8 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_auto_diff_node_forward.py::test_mul \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 12%]\u001b[0m\n",
      "tests/test_auto_diff_node_forward.py::test_mul_by_const \u001b[32mPASSED\u001b[0m\u001b[32m           [ 25%]\u001b[0m\n",
      "tests/test_auto_diff_node_forward.py::test_div \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 37%]\u001b[0m\n",
      "tests/test_auto_diff_node_forward.py::test_div_by_const \u001b[32mPASSED\u001b[0m\u001b[32m           [ 50%]\u001b[0m\n",
      "tests/test_auto_diff_node_forward.py::test_matmul[False-False] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 62%]\u001b[0m\n",
      "tests/test_auto_diff_node_forward.py::test_matmul[False-True] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 75%]\u001b[0m\n",
      "tests/test_auto_diff_node_forward.py::test_matmul[True-False] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 87%]\u001b[0m\n",
      "tests/test_auto_diff_node_forward.py::test_matmul[True-True] \u001b[32mPASSED\u001b[0m\u001b[32m      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.13s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v tests/test_auto_diff_node_forward.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x38qNHDK_h8"
   },
   "source": [
    "**Task 2 (15 pt).** Implement the `Executor.run` method in `auto_diff.py`.\n",
    "You may want to get the [topological sort](https://en.wikipedia.org/wiki/Topological_sorting) of the computational graph\n",
    "in order to compute the output value.\n",
    "\n",
    "We provide sample tests in `tests/test_auto_diff_graph_forward.py`.\n",
    "You can test your task 2 implementation by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1706414006440,
     "user": {
      "displayName": "Ruihang Lai",
      "userId": "03605576729730874720"
     },
     "user_tz": 300
    },
    "id": "pP92pGx2K_h8",
    "outputId": "9a034a75-19b5-4ef7-af62-1145c245bf6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.3.4, pluggy-1.5.0 -- /Users/tuomasier/miniconda3/envs/mlsys/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/tuomasier/Desktop/mlsys/assignment1\n",
      "plugins: anyio-4.6.2\n",
      "collected 12 items                                                             \u001b[0m\n",
      "\n",
      "tests/test_auto_diff_graph_forward.py::test_identity \u001b[32mPASSED\u001b[0m\u001b[32m              [  8%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_add \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 16%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_add_by_const \u001b[32mPASSED\u001b[0m\u001b[32m          [ 25%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_mul \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 33%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_mul_by_const \u001b[32mPASSED\u001b[0m\u001b[32m          [ 41%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_div \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 50%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_div_by_const \u001b[32mPASSED\u001b[0m\u001b[32m          [ 58%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_matmul[False-False] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 66%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_matmul[False-True] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 75%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_matmul[True-False] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 83%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_matmul[True-True] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 91%]\u001b[0m\n",
      "tests/test_auto_diff_graph_forward.py::test_graph \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m12 passed\u001b[0m\u001b[32m in 0.14s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v tests/test_auto_diff_graph_forward.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZx-3v2EK_h8"
   },
   "source": [
    "**Task 3 (15 pt).** Implement the `gradient` method for all ops in `auto_diff.py`. We provide the examples of `AddOp` and `AddByConstOp`, and you need to implement the rest.\n",
    "\n",
    "We provide sample tests in `tests/test_auto_diff_node_backward.py`.\n",
    "You can test your task 3 implementation by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1376,
     "status": "ok",
     "timestamp": 1706414009774,
     "user": {
      "displayName": "Ruihang Lai",
      "userId": "03605576729730874720"
     },
     "user_tz": 300
    },
    "id": "cuFjQYnZK_h8",
    "outputId": "414188a1-695a-4af3-cc68-4d88103fef16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.3.4, pluggy-1.5.0 -- /Users/tuomasier/miniconda3/envs/mlsys/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/tuomasier/Desktop/mlsys/assignment1\n",
      "plugins: anyio-4.6.2\n",
      "collected 6 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_auto_diff_node_backward.py::test_mul \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 16%]\u001b[0m\n",
      "tests/test_auto_diff_node_backward.py::test_div \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 33%]\u001b[0m\n",
      "tests/test_auto_diff_node_backward.py::test_matmul[False-False] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "tests/test_auto_diff_node_backward.py::test_matmul[False-True] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 66%]\u001b[0m\n",
      "tests/test_auto_diff_node_backward.py::test_matmul[True-False] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 83%]\u001b[0m\n",
      "tests/test_auto_diff_node_backward.py::test_matmul[True-True] \u001b[32mPASSED\u001b[0m\u001b[32m     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v tests/test_auto_diff_node_backward.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C96znygCK_h9"
   },
   "source": [
    "**Task 4 (20 pt).** Implement `gradients` function in `auto_diff.py`.\n",
    "You may also find topological sort helpful in the implementation.\n",
    "\n",
    "We provide sample tests in `tests/test_auto_diff_graph_backward.py`.\n",
    "You can test your task 4 implementation by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1706414011383,
     "user": {
      "displayName": "Ruihang Lai",
      "userId": "03605576729730874720"
     },
     "user_tz": 300
    },
    "id": "sIPbNc0UK_h9",
    "outputId": "cfd50c77-e18a-4c17-9c1e-d7ea0678ae83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.3.4, pluggy-1.5.0 -- /Users/tuomasier/miniconda3/envs/mlsys/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/tuomasier/Desktop/mlsys/assignment1\n",
      "plugins: anyio-4.6.2\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_auto_diff_graph_backward.py::test_graph \u001b[32mPASSED\u001b[0m\u001b[32m                [ 50%]\u001b[0m\n",
      "tests/test_auto_diff_graph_backward.py::test_gradient_of_gradient \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.10s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v tests/test_auto_diff_graph_backward.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-0ov5dha1Y3"
   },
   "source": [
    "### A few notes\n",
    "\n",
    "1. **Zero-rank arrays in NumPy.** As mentioned earlier, all values are assumed to have type `numpy.ndarray` throughout this assignment. One thing you may find interesting about NumPy is, if we add two zero-rank arrays together (e.g., `np.array(1) + np.array(2)`), it results in a scalar value, rather than a zero-rank array:\n",
    "```\n",
    ">>> x = np.array(1)\n",
    ">>> y = np.array(2)\n",
    ">>> type(x), type(y), x.ndim, y.ndim\n",
    "(<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, 0, 0)\n",
    ">>> z = x + y\n",
    ">>> z, type(z)\n",
    "(3, <class 'numpy.int64'>)\n",
    "```\n",
    "This means that, if you want to have a rigorous implementation of your assignment, you need to check the result type\n",
    "at the end of `compute` methods, and wraps any scalar values back to `numpy.ndarray`.\n",
    "However, for simplicity, we do not requrire you to do this, and it is completely up to you:\n",
    "there will be no test for this behavior, and you won't get fewer credits because of not doing this.\n",
    "Python by default also does not have eager type checking to throw error when you do not handle the scalars.\n",
    "\n",
    "2. **`Node.attrs`.** In the reference implementation of `AddByConstOp` in `auto_diff.py`, you will find that the `attrs` field is used to store the constant operand of the addition in the returned node. In general, the `attrs` field of a node stores all the **constants** that are known when constructing the computational graph: for the case of `AddByConstOp`, the constant operand is stored as a node attribute. While for general cases, an attribute does not have to be a node operand. You can see that in `MatMulOp` in `auto_diff.py`, we store the boolean flags denoting whether to transpose the input matrices as attributes. In the next part of this assignment, you may implement op like `SumOp`, and find it useful to store the axis being reduced as a node attribute.\n",
    "\n",
    "3. **Minimality of `gradients`.** The `gradients` function constructs the backward graph and returns the gradient nodes with regards to required nodes. One interesting note here is the minimality of the constructed backward graph. For example, for a graph of `y = x1 * x2 + x1`, if we are only interested in the gradient of `x1 * x2`, a minimal backward graph only contains the gradient node of `x1 * x2`, which means it is not necessary to construct the gradient nodes for `x1` and `x2`. In this assignment, we **do not** require you to construct the minimal backward graph, but it would be a good mental exercise to think about the possible pros/cons of constructing minimal backward graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaOoXdqQyfKV"
   },
   "source": [
    "## Part 2: SGD for logistic regression (40 pt)\n",
    "\n",
    "In this part, you need to implement the stochastic gradient descent (SGD) algorithm to train a simple logistic regression model.\n",
    "\n",
    "Specifically, for input $x\\in \\mathbb{R}^n$ , we'll consider a logistic regression model of the form\n",
    "$$z = W^T x+b$$\n",
    "where $W\\in \\mathbb{R}^{n\\times k}, b\\in \\mathbb{R}^k$ represent the weight and bias of modethe model, and $z\\in \\mathbb{R}^k$ represents the logits output by the network.\n",
    "\n",
    "The model should be trained with softmax / cross-entropy loss on mini-batches of training data,\n",
    "which means we want to solve the following optimization problem, under the mini-batch setting.\n",
    "\\begin{equation}\n",
    "\\min_{W, b} \\;\\; \\ell_{\\mathrm{softmax}}(XW+b, y),\n",
    "\\end{equation}\n",
    "where $X\\in \\mathbb{R}^{b \\times n}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U48mvZZQK_h9"
   },
   "source": [
    "### Your tasks\n",
    "\n",
    "In general, you need the following steps (components) to train the logistic regression model:\n",
    "\n",
    "**Task 5 (15 pt).** Define the forward computational graph for $Z = XW+b$ in `logistic_regression` function in `logistic_regression.py`.\n",
    "Note that $XW$ is a 2-dim matrix, while $b$ is a 1-dim vector.\n",
    "You may find it helpful to introduce a new operator that broadcasts the $b$ vector to the matrix shape (think about why?).\n",
    "In common frameworks, people use `broadcast_to`, e.g., [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.broadcast_to.html),\n",
    "for this purpose. However, our computational graph nodes do not carry shape information.\n",
    "So you may want to slightly tweak the interface of your own broadcasting op."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vBP4Qb3K_h-"
   },
   "source": [
    "**Task 6 (15 pt).** Implement `softmax_loss` function in `logistic_regression.py` that constructs the computational graph of softmax loss.\n",
    "The softmax loss takes an input node of logits and a node of one-hot encodings of the true labels.\n",
    "As a reminder, for a multi-class output that can take on values $y \\in \\{1,\\ldots,k\\}$,\n",
    "the softmax loss takes a vector of logits $z \\in \\mathbb{R}^k$ and the true class $y \\in \\{1,\\ldots,k\\}$ (which is encoded for this function as a one-hot vector),\n",
    "and returns a loss defined by\n",
    "\\begin{equation}\n",
    "\\ell_{\\mathrm{softmax}}(z, y) = \\log\\sum_{i=1}^k \\exp z_i - z_y.\n",
    "\\end{equation}\n",
    "You may need to introduce new operators to compute summation, logarithm, exponentiation and their gradients to build up softmax loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjyrNstkK_h-"
   },
   "source": [
    "**Task 7 (10 pt).** Implement `sgd_epoch` function in `logistic_regression.py` to run a single epoch of SGD.\n",
    "In this function, you need to split the input data and labels into several mini-batches.\n",
    "Then run the constructed computational graph given one batch as input.\n",
    "Collect gradients and update the weight/bias of your logistic regression model correspondingly.\n",
    "\n",
    "If your implementation is correct, you will observe that the prediction accuracy on the handwritten digit dataset is around 95% by running `logistic_regression.py`:\n",
    "```shell\n",
    "> python3 logistic_regression.py\n",
    "...\n",
    "Final test accuracy: 0.9611111111111111\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(sum((log(sum(exp(((x@W)+broadcast(b -> (x@W)))),axis=1,keepdims=True))+(sum((((x@W)+broadcast(b -> (x@W)))*y),axis=1,keepdims=True)*-1)),axis=None,keepdims=False)/50), sum((log(sum(exp(((x@W)+broadcast(b -> (x@W)))),axis=1,keepdims=True))+(sum((((x@W)+broadcast(b -> (x@W)))*y),axis=1,keepdims=True)*-1)),axis=None,keepdims=False), (log(sum(exp(((x@W)+broadcast(b -> (x@W)))),axis=1,keepdims=True))+(sum((((x@W)+broadcast(b -> (x@W)))*y),axis=1,keepdims=True)*-1)), (sum((((x@W)+broadcast(b -> (x@W)))*y),axis=1,keepdims=True)*-1), sum((((x@W)+broadcast(b -> (x@W)))*y),axis=1,keepdims=True), (((x@W)+broadcast(b -> (x@W)))*y), y, log(sum(exp(((x@W)+broadcast(b -> (x@W)))),axis=1,keepdims=True)), sum(exp(((x@W)+broadcast(b -> (x@W)))),axis=1,keepdims=True), exp(((x@W)+broadcast(b -> (x@W)))), ((x@W)+broadcast(b -> (x@W))), broadcast(b -> (x@W)), b, (x@W), W, x]\n",
      "Epoch 0: test accuracy = 0.8972222222222223, loss = 8.263342398024955\n",
      "Epoch 1: test accuracy = 0.8444444444444444, loss = 0.7578445958277097\n",
      "Epoch 2: test accuracy = 0.9138888888888889, loss = 0.4268214288863778\n",
      "Epoch 3: test accuracy = 0.9388888888888889, loss = 0.29096330730717523\n",
      "Epoch 4: test accuracy = 0.9388888888888889, loss = 0.3081577568763294\n",
      "Epoch 5: test accuracy = 0.9444444444444444, loss = 0.17348875641139394\n",
      "Epoch 6: test accuracy = 0.925, loss = 0.26430484603747895\n",
      "Epoch 7: test accuracy = 0.9444444444444444, loss = 0.1284162449698565\n",
      "Epoch 8: test accuracy = 0.9472222222222222, loss = 0.13342112325649783\n",
      "Epoch 9: test accuracy = 0.9527777777777777, loss = 0.17677630895818078\n",
      "Epoch 10: test accuracy = 0.9527777777777777, loss = 0.10401185378542606\n",
      "Epoch 11: test accuracy = 0.9333333333333333, loss = 0.09385331433899403\n",
      "Epoch 12: test accuracy = 0.9416666666666667, loss = 0.0681018389936467\n",
      "Epoch 13: test accuracy = 0.9444444444444444, loss = 0.064377448425916\n",
      "Epoch 14: test accuracy = 0.9388888888888889, loss = 0.05252152275967248\n",
      "Epoch 15: test accuracy = 0.9472222222222222, loss = 0.04252093424079094\n",
      "Epoch 16: test accuracy = 0.9138888888888889, loss = 0.04037292329678585\n",
      "Epoch 17: test accuracy = 0.9555555555555556, loss = 0.03965920429926995\n",
      "Epoch 18: test accuracy = 0.9555555555555556, loss = 0.042776577061491385\n",
      "Epoch 19: test accuracy = 0.9611111111111111, loss = 0.046069045718815996\n",
      "Epoch 20: test accuracy = 0.95, loss = 0.03212144348855965\n",
      "Epoch 21: test accuracy = 0.9444444444444444, loss = 0.03203236501623035\n",
      "Epoch 22: test accuracy = 0.9444444444444444, loss = 0.029448164503576305\n",
      "Epoch 23: test accuracy = 0.95, loss = 0.02924299433239957\n",
      "Epoch 24: test accuracy = 0.9555555555555556, loss = 0.02821315837003993\n",
      "Epoch 25: test accuracy = 0.9472222222222222, loss = 0.02386774132882444\n",
      "Epoch 26: test accuracy = 0.9583333333333334, loss = 0.024440642187867984\n",
      "Epoch 27: test accuracy = 0.9527777777777777, loss = 0.018683166980287047\n",
      "Epoch 28: test accuracy = 0.9555555555555556, loss = 0.01995000472802314\n",
      "Epoch 29: test accuracy = 0.9555555555555556, loss = 0.016184391679118306\n",
      "Epoch 30: test accuracy = 0.9472222222222222, loss = 0.027657378710950545\n",
      "Epoch 31: test accuracy = 0.9527777777777777, loss = 0.01867681027741312\n",
      "Epoch 32: test accuracy = 0.95, loss = 0.01349060271313111\n",
      "Epoch 33: test accuracy = 0.9527777777777777, loss = 0.016338822939160856\n",
      "Epoch 34: test accuracy = 0.9555555555555556, loss = 0.012211777683208542\n",
      "Epoch 35: test accuracy = 0.9527777777777777, loss = 0.014305613604188742\n",
      "Epoch 36: test accuracy = 0.9555555555555556, loss = 0.00948287515571955\n",
      "Epoch 37: test accuracy = 0.9583333333333334, loss = 0.012217783889210711\n",
      "Epoch 38: test accuracy = 0.9611111111111111, loss = 0.01148131528404136\n",
      "Epoch 39: test accuracy = 0.9583333333333334, loss = 0.011137924490539401\n",
      "Epoch 40: test accuracy = 0.9555555555555556, loss = 0.011088193367960053\n",
      "Epoch 41: test accuracy = 0.9583333333333334, loss = 0.010446175325990086\n",
      "Epoch 42: test accuracy = 0.9555555555555556, loss = 0.008784531848932147\n",
      "Epoch 43: test accuracy = 0.9583333333333334, loss = 0.008889841962395093\n",
      "Epoch 44: test accuracy = 0.9527777777777777, loss = 0.011037488235672337\n",
      "Epoch 45: test accuracy = 0.9583333333333334, loss = 0.008347687386743588\n",
      "Epoch 46: test accuracy = 0.9583333333333334, loss = 0.007527767651351867\n",
      "Epoch 47: test accuracy = 0.9555555555555556, loss = 0.009284518185618146\n",
      "Epoch 48: test accuracy = 0.9583333333333334, loss = 0.009014530741525735\n",
      "Epoch 49: test accuracy = 0.9611111111111111, loss = 0.0068267885486061955\n",
      "Epoch 50: test accuracy = 0.9555555555555556, loss = 0.0069677432359971795\n",
      "Epoch 51: test accuracy = 0.9527777777777777, loss = 0.006105812696168919\n",
      "Epoch 52: test accuracy = 0.9583333333333334, loss = 0.0070454731477309785\n",
      "Epoch 53: test accuracy = 0.9611111111111111, loss = 0.006628473398489597\n",
      "Epoch 54: test accuracy = 0.9444444444444444, loss = 0.006086200208476882\n",
      "Epoch 55: test accuracy = 0.9611111111111111, loss = 0.006422523276338142\n",
      "Epoch 56: test accuracy = 0.9527777777777777, loss = 0.006256030931725422\n",
      "Epoch 57: test accuracy = 0.9583333333333334, loss = 0.005701858758791658\n",
      "Epoch 58: test accuracy = 0.9611111111111111, loss = 0.00619805971620925\n",
      "Epoch 59: test accuracy = 0.9611111111111111, loss = 0.005893299856631897\n",
      "Epoch 60: test accuracy = 0.9611111111111111, loss = 0.006779974521754785\n",
      "Epoch 61: test accuracy = 0.9611111111111111, loss = 0.0054928374145677685\n",
      "Epoch 62: test accuracy = 0.9555555555555556, loss = 0.0057834603663094175\n",
      "Epoch 63: test accuracy = 0.9583333333333334, loss = 0.005079934710778524\n",
      "Epoch 64: test accuracy = 0.9583333333333334, loss = 0.006444341048938226\n",
      "Epoch 65: test accuracy = 0.9583333333333334, loss = 0.004813261670871297\n",
      "Epoch 66: test accuracy = 0.9611111111111111, loss = 0.0056962714494547696\n",
      "Epoch 67: test accuracy = 0.9611111111111111, loss = 0.005361351798893657\n",
      "Epoch 68: test accuracy = 0.9611111111111111, loss = 0.004888607605467739\n",
      "Epoch 69: test accuracy = 0.9611111111111111, loss = 0.005566231629448427\n",
      "Epoch 70: test accuracy = 0.9611111111111111, loss = 0.004641168907535709\n",
      "Epoch 71: test accuracy = 0.9583333333333334, loss = 0.004999099221075027\n",
      "Epoch 72: test accuracy = 0.9611111111111111, loss = 0.0051531794376081606\n",
      "Epoch 73: test accuracy = 0.9611111111111111, loss = 0.004694260877098567\n",
      "Epoch 74: test accuracy = 0.9611111111111111, loss = 0.00473316185180019\n",
      "Epoch 75: test accuracy = 0.9611111111111111, loss = 0.004159880233927038\n",
      "Epoch 76: test accuracy = 0.9611111111111111, loss = 0.00485043791297973\n",
      "Epoch 77: test accuracy = 0.9583333333333334, loss = 0.0049464930442452495\n",
      "Epoch 78: test accuracy = 0.9583333333333334, loss = 0.004147866456591926\n",
      "Epoch 79: test accuracy = 0.9611111111111111, loss = 0.004460851107192631\n",
      "Epoch 80: test accuracy = 0.9611111111111111, loss = 0.004814351457618025\n",
      "Epoch 81: test accuracy = 0.9611111111111111, loss = 0.004602734310896694\n",
      "Epoch 82: test accuracy = 0.9583333333333334, loss = 0.004531140651743641\n",
      "Epoch 83: test accuracy = 0.9611111111111111, loss = 0.004082534338352036\n",
      "Epoch 84: test accuracy = 0.9611111111111111, loss = 0.005044127342621544\n",
      "Epoch 85: test accuracy = 0.9611111111111111, loss = 0.004345514919905069\n",
      "Epoch 86: test accuracy = 0.9583333333333334, loss = 0.003966290050439721\n",
      "Epoch 87: test accuracy = 0.9611111111111111, loss = 0.005202133142014494\n",
      "Epoch 88: test accuracy = 0.9638888888888889, loss = 0.010791065639950302\n",
      "Epoch 89: test accuracy = 0.9638888888888889, loss = 0.004447629623036024\n",
      "Epoch 90: test accuracy = 0.9583333333333334, loss = 0.004294635444384113\n",
      "Epoch 91: test accuracy = 0.9611111111111111, loss = 0.005028332149898993\n",
      "Epoch 92: test accuracy = 0.9611111111111111, loss = 0.004666715837266857\n",
      "Epoch 93: test accuracy = 0.9611111111111111, loss = 0.003819627401051363\n",
      "Epoch 94: test accuracy = 0.9638888888888889, loss = 0.0038226530099954165\n",
      "Epoch 95: test accuracy = 0.9611111111111111, loss = 0.003923230355342222\n",
      "Epoch 96: test accuracy = 0.9611111111111111, loss = 0.003907531807722764\n",
      "Epoch 97: test accuracy = 0.9583333333333334, loss = 0.0037211252489182493\n",
      "Epoch 98: test accuracy = 0.9583333333333334, loss = 0.0038503283606484665\n",
      "Epoch 99: test accuracy = 0.9611111111111111, loss = 0.003730186428856797\n",
      "Final test accuracy: 0.9611111111111111\n"
     ]
    }
   ],
   "source": [
    "!python3 logistic_regression.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1vY5jDUK_h-"
   },
   "source": [
    "**Hint.** When you find the current op set not satisfying your needs, consider introducing a new op."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcGILNQ2K_h_"
   },
   "source": [
    "## Part 3. Create Your Own Test Cases (0 pt)\n",
    "\n",
    "We encourage you to create your own test cases, which helps you confirm the correctness of your implementation.\n",
    "If you are interested, you can write your own tests in `tests/test_customized_cases.py` and share them with us by including this file in your submission.\n",
    "We appreciate it if you can share your tests, which can help improve this course and the assignment. Please note that this part is voluntary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.3.4, pluggy-1.5.0 -- /Users/tuomasier/miniconda3/envs/mlsys/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/tuomasier/Desktop/mlsys/assignment1\n",
      "plugins: anyio-4.6.2\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_customized_cases.py::test_graph \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 50%]\u001b[0m\n",
      "tests/test_customized_cases.py::test_gradient_of_gradient \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v tests/test_customized_cases.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfTn0wD0K_h_"
   },
   "source": [
    "## Part 4. Assignment Feedback (0 pt)\n",
    "\n",
    "This is the second time we offer this course, and we appreciate any assignment feedback from you.\n",
    "You can leave your feedback (if any) in `feedback.txt`, and submit it together with the source code.\n",
    "Possible choices can be:\n",
    "\n",
    "- How difficult do you think this assignment is?\n",
    "- How much time does the assignment take? Which task takes the most time?\n",
    "- Which part of the assignment do you feel hard to understand?\n",
    "- And any other things you would like to share.\n",
    "\n",
    "Your feedback will be very useful in helping us improve the assignment quality\n",
    "for next years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83n9F8uYK_h_"
   },
   "source": [
    "## How to Submit Your Code\n",
    "\n",
    "In the home directory for the assignment, execute the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip handin.zip auto_diff.py logistic_regression.py tests/test_customized_cases.py feedback.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a zip file with `auto_diff.py`, `logistic_regression.py`, `tests/test_customized_cases.py` and `feedback.txt`.\n",
    "You can check the contents of `handin.zip` to make sure it contains all the needed files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zipinfo -1 handin.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected to list the four files:\n",
    "```\n",
    "auto_diff.py\n",
    "logistic_regression.py\n",
    "tests/test_customized_cases.py\n",
    "feedback.txt\n",
    "```\n",
    "\n",
    "Then, please go to GradeScope at https://www.gradescope.com/courses/951055 and submit the file `handin.zip` to Assignment 1.\n",
    "\n",
    "The assignment will be automatically graded. The test cases include both public tests that we provide under `tests/`,\n",
    "as well as some private tests (which will not be disclosed).\n",
    "You can submit multiple times, and the time stamp of that submission will be used in determining any late penalties.\n",
    "Please make sure that your submitted `auto_diff.py` and `logistic_regression.py` are placed at the root level of the zip file (i.e., they are not in any sub-folder),\n",
    "or **otherwise the autograder may not process your submission properly**.\n",
    "\n",
    "**Any attempt to manipulate or compromise the integrity of the autograder will result in severe penalties.**\n",
    "\n",
    "\n",
    "If you are enrolled in the course (on SIO), but not registered on Gradescope, please let the course staff know in a private post on Piazza."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/mlsyscourse/assignment1/blob/main/mlsys_hw1.ipynb",
     "timestamp": 1706413758241
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.13.2 ('mlsys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6221bec73ab659bcdbf6a5ad67c31df15de116f3f0518bdf90a3d58d5efa213"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
